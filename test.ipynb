{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PBFJZ1wemssm"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss functions and gradients\n",
        "class Loss:\n",
        "    @staticmethod\n",
        "    def cross_entropy(y_true, y_pred):\n",
        "        # Clip values to avoid log(0) issues\n",
        "        epsilon = 1e-12\n",
        "        y_pred = np.clip(y_pred, epsilon, 1. - epsilon)\n",
        "        loss = -np.sum(y_true * np.log(y_pred))\n",
        "        return loss\n",
        "\n",
        "    @staticmethod\n",
        "    def cross_entropy_gradient(y_true, y_pred):\n",
        "        return y_pred - y_true\n",
        "\n",
        "# Activation functions and derivatives\n",
        "class Activation:\n",
        "    @staticmethod\n",
        "    def relu(z):\n",
        "        return np.maximum(0, z)\n",
        "\n",
        "    @staticmethod\n",
        "    def relu_derivative(z):\n",
        "        return np.where(z > 0, 1, 0)\n",
        "\n",
        "    @staticmethod\n",
        "    def softmax(z):\n",
        "        # Subtract max for numerical stability\n",
        "        z -= np.max(z, axis=1, keepdims=True)\n",
        "        exp_vals = np.exp(z)\n",
        "        return exp_vals / np.sum(exp_vals, axis=1, keepdims=True)\n",
        "\n",
        "    # For softmax used with cross-entropy, the derivative is taken care of in the loss gradient\n",
        "\n",
        "# Fully connected layer definition\n",
        "class Layer:\n",
        "    def __init__(self, num_in, num_out, activation, activation_deriv, learning_rate=0.01):\n",
        "        rng = np.random.default_rng(42)\n",
        "        # He initialization for layers using ReLU\n",
        "        self.weights = rng.standard_normal((num_in, num_out)) * np.sqrt(2 / num_in)\n",
        "        self.biases = np.zeros((1, num_out))\n",
        "        self.activation = activation\n",
        "        self.activation_deriv = activation_deriv\n",
        "        self.learning_rate = learning_rate\n",
        "        self.inputs = None\n",
        "        self.z = None\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        self.inputs = inputs\n",
        "        self.z = np.dot(inputs, self.weights) + self.biases\n",
        "        return self.activation(self.z)\n",
        "\n",
        "    def backward(self, dl_dy):\n",
        "        # For layers with ReLU or similar activations\n",
        "        if self.activation_deriv is not None:\n",
        "            dl_dz = dl_dy * self.activation_deriv(self.z)\n",
        "        else:\n",
        "            dl_dz = dl_dy\n",
        "        dl_dw = self.inputs.T @ dl_dz\n",
        "        dl_db = np.sum(dl_dz, axis=0, keepdims=True)\n",
        "        dl_dx = np.dot(dl_dz, self.weights.T)\n",
        "\n",
        "        batch_size = self.inputs.shape[0]\n",
        "        self.weights -= self.learning_rate * (dl_dw / batch_size)\n",
        "        self.biases  -= self.learning_rate * (dl_db / batch_size)\n",
        "        return dl_dx\n"
      ],
      "metadata": {
        "id": "3odHkxfxvTEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard one-hot encoding function\n",
        "def one_hot_encode(num_classes, y):\n",
        "    return np.eye(num_classes)[y]"
      ],
      "metadata": {
        "id": "gz7LM0PbpMlo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/train.csv')"
      ],
      "metadata": {
        "id": "GhEjHdEHsade"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "r7hM0-noEkvC",
        "outputId": "f4a57189-041c-469f-99ae-9f1f051011ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Survived  Pclass  Sex   Age  SibSp  Parch     Fare Embarked\n",
              "0         0       3    0  22.0      1      0   7.2500        S\n",
              "1         1       1    1  38.0      1      0  71.2833        C\n",
              "2         1       3    1  26.0      0      0   7.9250        S\n",
              "3         1       1    1  35.0      1      0  53.1000        S\n",
              "4         0       3    0  35.0      0      0   8.0500        S"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4c029fb9-b875-4e9e-ba1e-19fc86976190\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Survived</th>\n",
              "      <th>Pclass</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Age</th>\n",
              "      <th>SibSp</th>\n",
              "      <th>Parch</th>\n",
              "      <th>Fare</th>\n",
              "      <th>Embarked</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>7.2500</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>38.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>71.2833</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7.9250</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>35.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>53.1000</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>8.0500</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4c029fb9-b875-4e9e-ba1e-19fc86976190')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-4c029fb9-b875-4e9e-ba1e-19fc86976190 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-4c029fb9-b875-4e9e-ba1e-19fc86976190');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-63ee899e-0da4-4cff-afa1-118f7c408cc0\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-63ee899e-0da4-4cff-afa1-118f7c408cc0')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-63ee899e-0da4-4cff-afa1-118f7c408cc0 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 712,\n  \"fields\": [\n    {\n      \"column\": \"Survived\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Pclass\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 3,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          3,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sex\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Age\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 14.49293290032352,\n        \"min\": 0.42,\n        \"max\": 80.0,\n        \"num_unique_values\": 88,\n        \"samples\": [\n          0.75,\n          22.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"SibSp\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 5,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Parch\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 6,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Fare\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 52.938648174710885,\n        \"min\": 0.0,\n        \"max\": 512.3292,\n        \"num_unique_values\": 219,\n        \"samples\": [\n          7.875,\n          146.5208\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Embarked\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"S\",\n          \"C\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop(columns=[\"PassengerId\", \"Name\", \"Ticket\", \"Cabin\"])"
      ],
      "metadata": {
        "id": "fDX2VuZuFJ6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Sex\"] = df[\"Sex\"].map({\"male\" : 0, \"female\": 1})"
      ],
      "metadata": {
        "id": "s5WHFkS0P50H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.dropna(axis=0, how=\"any\", inplace=True)"
      ],
      "metadata": {
        "id": "3q4ygVrmpZM9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Embarked\"] = df[\"Embarked\"].map({\"S\": 0, \"C\":1 , \"Q\": 2})"
      ],
      "metadata": {
        "id": "kOctyZpPqQ7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.drop(columns=[\"Survived\"]).values\n",
        "y = df[\"Survived\"].values"
      ],
      "metadata": {
        "id": "JwzOSEl-RKgj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "3xsZ9SA3uWg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Y7Lv4_txRQbt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 42)"
      ],
      "metadata": {
        "id": "6XdgHNoXtB8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_encoded = one_hot_encode(2, y_train)\n",
        "y_test_encoded = one_hot_encode(2, y_test)\n",
        "# Define the model with two output neurons\n",
        "model = [\n",
        "    Layer(X_train.shape[1], 8, Activation.relu, Activation.relu_derivative),\n",
        "    Layer(8, 8, Activation.relu, Activation.relu_derivative),\n",
        "    Layer(8, 2, lambda x: x, lambda x: np.ones_like(x))\n",
        "]\n",
        "\n",
        "# Training loop\n",
        "EPOCHS = 1000\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    epoch_loss = 0\n",
        "    num_batches = 0\n",
        "    for batch_idx in range(0, len(X_train), BATCH_SIZE):\n",
        "        X_batch = X_train[batch_idx:batch_idx+BATCH_SIZE]\n",
        "        y_batch = y_train_encoded[batch_idx:batch_idx+BATCH_SIZE]\n",
        "\n",
        "        # Forward propagation\n",
        "        output = X_batch\n",
        "        for layer in model:\n",
        "            output = layer.forward(output)\n",
        "\n",
        "        # Compute loss and its derivative\n",
        "        loss = Loss.cross_entropy(y_batch, output)\n",
        "        loss_der = Loss.cross_entropy_gradient(y_batch, output)\n",
        "\n",
        "        # Backward propagation\n",
        "        for layer in reversed(model):\n",
        "            loss_der = layer.backward(loss_der)\n",
        "\n",
        "        epoch_loss += np.mean(loss)\n",
        "        num_batches += 1\n",
        "\n",
        "    print(f\"Epoch {epoch+1} loss: {epoch_loss / num_batches}\")\n"
      ],
      "metadata": {
        "id": "AD9nYcRZyzjJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ab1b727-b63c-430c-eeac-9f953952722b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 loss: 142.31318817227637\n",
            "Epoch 2 loss: 34.65362090983652\n",
            "Epoch 3 loss: 19.88474286274528\n",
            "Epoch 4 loss: 17.034544603410712\n",
            "Epoch 5 loss: 14.950383810154314\n",
            "Epoch 6 loss: 14.47086487780345\n",
            "Epoch 7 loss: 14.038791783307309\n",
            "Epoch 8 loss: 13.687237349776337\n",
            "Epoch 9 loss: 12.821593013014253\n",
            "Epoch 10 loss: 12.580629388904669\n",
            "Epoch 11 loss: 12.969425980924882\n",
            "Epoch 12 loss: 12.752782260427182\n",
            "Epoch 13 loss: 13.206617041126552\n",
            "Epoch 14 loss: 13.010992410168951\n",
            "Epoch 15 loss: 12.864835042782529\n",
            "Epoch 16 loss: 13.306642254510688\n",
            "Epoch 17 loss: 13.15893282784201\n",
            "Epoch 18 loss: 13.029133142733013\n",
            "Epoch 19 loss: 12.911564990452549\n",
            "Epoch 20 loss: 12.800149409376107\n",
            "Epoch 21 loss: 12.694857196106916\n",
            "Epoch 22 loss: 12.606347225000807\n",
            "Epoch 23 loss: 12.517940424626028\n",
            "Epoch 24 loss: 12.435616066197994\n",
            "Epoch 25 loss: 12.363134277793622\n",
            "Epoch 26 loss: 12.30199718190059\n",
            "Epoch 27 loss: 11.00327716440711\n",
            "Epoch 28 loss: 10.900523336119669\n",
            "Epoch 29 loss: 10.8250315398416\n",
            "Epoch 30 loss: 10.763902049870964\n",
            "Epoch 31 loss: 10.710430249958355\n",
            "Epoch 32 loss: 10.664105956455357\n",
            "Epoch 33 loss: 10.622340839476712\n",
            "Epoch 34 loss: 9.945646533818909\n",
            "Epoch 35 loss: 9.892838820281566\n",
            "Epoch 36 loss: 9.851756274285142\n",
            "Epoch 37 loss: 9.816376096813137\n",
            "Epoch 38 loss: 9.784900420000548\n",
            "Epoch 39 loss: 9.135931633332106\n",
            "Epoch 40 loss: 8.437330345639635\n",
            "Epoch 41 loss: 8.365280497074245\n",
            "Epoch 42 loss: 8.314140106230509\n",
            "Epoch 43 loss: 8.271318078681993\n",
            "Epoch 44 loss: 8.232024631464174\n",
            "Epoch 45 loss: 7.581683152397899\n",
            "Epoch 46 loss: 7.520527817590287\n",
            "Epoch 47 loss: 7.477113485660218\n",
            "Epoch 48 loss: 7.440653293556407\n",
            "Epoch 49 loss: 7.407460302292999\n",
            "Epoch 50 loss: 7.377254144246226\n",
            "Epoch 51 loss: 7.3494609535246225\n",
            "Epoch 52 loss: 7.324785851265143\n",
            "Epoch 53 loss: 7.3044374943945\n",
            "Epoch 54 loss: 7.285266192773018\n",
            "Epoch 55 loss: 7.2680102455003555\n",
            "Epoch 56 loss: 7.251892087612545\n",
            "Epoch 57 loss: 7.239207099011399\n",
            "Epoch 58 loss: 7.2287916555910385\n",
            "Epoch 59 loss: 7.220982596019411\n",
            "Epoch 60 loss: 7.214707078065179\n",
            "Epoch 61 loss: 7.204323332238715\n",
            "Epoch 62 loss: 7.209830707263573\n",
            "Epoch 63 loss: 7.77247592301564\n",
            "Epoch 64 loss: 7.7571644162148345\n",
            "Epoch 65 loss: 7.7427213667305335\n",
            "Epoch 66 loss: 7.727845095071724\n",
            "Epoch 67 loss: 7.713593423679374\n",
            "Epoch 68 loss: 7.700292569942549\n",
            "Epoch 69 loss: 7.686182880138192\n",
            "Epoch 70 loss: 7.6731186437912084\n",
            "Epoch 71 loss: 7.661871338845531\n",
            "Epoch 72 loss: 7.650611229377991\n",
            "Epoch 73 loss: 7.639695946057965\n",
            "Epoch 74 loss: 7.628925709707198\n",
            "Epoch 75 loss: 7.618304847589473\n",
            "Epoch 76 loss: 7.607840644942586\n",
            "Epoch 77 loss: 7.597369812123514\n",
            "Epoch 78 loss: 7.586541446648665\n",
            "Epoch 79 loss: 7.57720599252331\n",
            "Epoch 80 loss: 7.56823890187268\n",
            "Epoch 81 loss: 7.559058047954874\n",
            "Epoch 82 loss: 7.548501958574708\n",
            "Epoch 83 loss: 7.5381773382492225\n",
            "Epoch 84 loss: 7.529028802146417\n",
            "Epoch 85 loss: 7.5199796083223855\n",
            "Epoch 86 loss: 7.511922059269407\n",
            "Epoch 87 loss: 7.503663270375999\n",
            "Epoch 88 loss: 7.496242973113856\n",
            "Epoch 89 loss: 7.4890526935657045\n",
            "Epoch 90 loss: 7.4822403236828245\n",
            "Epoch 91 loss: 7.475313115278193\n",
            "Epoch 92 loss: 7.468393465960798\n",
            "Epoch 93 loss: 7.459468069958032\n",
            "Epoch 94 loss: 7.453720583543433\n",
            "Epoch 95 loss: 7.447121225478398\n",
            "Epoch 96 loss: 7.440446359208794\n",
            "Epoch 97 loss: 7.434279415128123\n",
            "Epoch 98 loss: 7.427297968913174\n",
            "Epoch 99 loss: 7.42307504527373\n",
            "Epoch 100 loss: 7.418735850506481\n",
            "Epoch 101 loss: 7.413244444660042\n",
            "Epoch 102 loss: 7.408422771084322\n",
            "Epoch 103 loss: 7.401707928307271\n",
            "Epoch 104 loss: 7.397330212833868\n",
            "Epoch 105 loss: 7.392780923945584\n",
            "Epoch 106 loss: 7.385885029720332\n",
            "Epoch 107 loss: 7.381918174979906\n",
            "Epoch 108 loss: 7.374955536962235\n",
            "Epoch 109 loss: 7.370885047252171\n",
            "Epoch 110 loss: 7.365228135100811\n",
            "Epoch 111 loss: 7.359041038784535\n",
            "Epoch 112 loss: 7.3543728639899415\n",
            "Epoch 113 loss: 7.34869295609251\n",
            "Epoch 114 loss: 7.344353295999808\n",
            "Epoch 115 loss: 7.338832631047272\n",
            "Epoch 116 loss: 7.332887472387256\n",
            "Epoch 117 loss: 7.328512675370634\n",
            "Epoch 118 loss: 7.322555765071588\n",
            "Epoch 119 loss: 7.318497489237476\n",
            "Epoch 120 loss: 7.314977524375641\n",
            "Epoch 121 loss: 7.310639042092292\n",
            "Epoch 122 loss: 7.308484447730515\n",
            "Epoch 123 loss: 7.3060296471611235\n",
            "Epoch 124 loss: 7.300979333331957\n",
            "Epoch 125 loss: 7.298589636570965\n",
            "Epoch 126 loss: 7.294599708680282\n",
            "Epoch 127 loss: 7.291339548434327\n",
            "Epoch 128 loss: 7.286103643701193\n",
            "Epoch 129 loss: 7.283553104001866\n",
            "Epoch 130 loss: 7.279499851062234\n",
            "Epoch 131 loss: 7.274992227741691\n",
            "Epoch 132 loss: 7.271404030170705\n",
            "Epoch 133 loss: 7.268186217571456\n",
            "Epoch 134 loss: 7.264979565936094\n",
            "Epoch 135 loss: 7.260155061171976\n",
            "Epoch 136 loss: 7.257076193154802\n",
            "Epoch 137 loss: 7.255504472460162\n",
            "Epoch 138 loss: 7.252581582398864\n",
            "Epoch 139 loss: 7.249454954630226\n",
            "Epoch 140 loss: 7.248394869666902\n",
            "Epoch 141 loss: 7.245450282537245\n",
            "Epoch 142 loss: 7.243565356029537\n",
            "Epoch 143 loss: 7.243142255242471\n",
            "Epoch 144 loss: 7.24040689777832\n",
            "Epoch 145 loss: 7.2395300372158085\n",
            "Epoch 146 loss: 7.239817087788737\n",
            "Epoch 147 loss: 7.239251431068308\n",
            "Epoch 148 loss: 7.238232240439138\n",
            "Epoch 149 loss: 7.239322616543114\n",
            "Epoch 150 loss: 7.239937536254762\n",
            "Epoch 151 loss: 7.241423499912148\n",
            "Epoch 152 loss: 7.244509004314373\n",
            "Epoch 153 loss: 7.243530530808248\n",
            "Epoch 154 loss: 7.251905572186589\n",
            "Epoch 155 loss: 7.259089660187263\n",
            "Epoch 156 loss: 7.2784727164281104\n",
            "Epoch 157 loss: 7.31424100169855\n",
            "Epoch 158 loss: 7.855146538855245\n",
            "Epoch 159 loss: 7.852873568237076\n",
            "Epoch 160 loss: 7.320403985057101\n",
            "Epoch 161 loss: 7.278713414923022\n",
            "Epoch 162 loss: 7.2567265151189355\n",
            "Epoch 163 loss: 7.251760596392016\n",
            "Epoch 164 loss: 7.2384163010759135\n",
            "Epoch 165 loss: 7.228891608529547\n",
            "Epoch 166 loss: 7.221202622651891\n",
            "Epoch 167 loss: 7.212976181998463\n",
            "Epoch 168 loss: 7.210387045164189\n",
            "Epoch 169 loss: 7.205090710239779\n",
            "Epoch 170 loss: 7.199874918032847\n",
            "Epoch 171 loss: 7.19490612157141\n",
            "Epoch 172 loss: 7.189751280676739\n",
            "Epoch 173 loss: 7.18948664313648\n",
            "Epoch 174 loss: 7.184070514130282\n",
            "Epoch 175 loss: 7.179454568658509\n",
            "Epoch 176 loss: 7.174574475216265\n",
            "Epoch 177 loss: 7.173380726927722\n",
            "Epoch 178 loss: 7.169268814541961\n",
            "Epoch 179 loss: 7.166499566047203\n",
            "Epoch 180 loss: 7.16164781599554\n",
            "Epoch 181 loss: 7.161096167264716\n",
            "Epoch 182 loss: 7.158643079804506\n",
            "Epoch 183 loss: 7.155740497382378\n",
            "Epoch 184 loss: 7.151441631120095\n",
            "Epoch 185 loss: 7.150866621390875\n",
            "Epoch 186 loss: 7.148971941992094\n",
            "Epoch 187 loss: 7.146397549348915\n",
            "Epoch 188 loss: 7.145441905487926\n",
            "Epoch 189 loss: 7.142184543228041\n",
            "Epoch 190 loss: 7.13729621245335\n",
            "Epoch 191 loss: 7.136767713111275\n",
            "Epoch 192 loss: 7.133044875015271\n",
            "Epoch 193 loss: 7.1280750977897815\n",
            "Epoch 194 loss: 7.126107472087355\n",
            "Epoch 195 loss: 7.122217866327391\n",
            "Epoch 196 loss: 7.119061897898991\n",
            "Epoch 197 loss: 7.115841442883042\n",
            "Epoch 198 loss: 7.112200395548322\n",
            "Epoch 199 loss: 7.108219792611074\n",
            "Epoch 200 loss: 7.104682343917744\n",
            "Epoch 201 loss: 7.100939225543804\n",
            "Epoch 202 loss: 7.096484451749276\n",
            "Epoch 203 loss: 7.095421419189588\n",
            "Epoch 204 loss: 7.092380973173518\n",
            "Epoch 205 loss: 7.088767029394742\n",
            "Epoch 206 loss: 7.084929775394764\n",
            "Epoch 207 loss: 7.082144069276793\n",
            "Epoch 208 loss: 7.079430443001688\n",
            "Epoch 209 loss: 7.0748012268080025\n",
            "Epoch 210 loss: 7.074581948144552\n",
            "Epoch 211 loss: 7.072326662629237\n",
            "Epoch 212 loss: 7.068816002769476\n",
            "Epoch 213 loss: 7.064428568047698\n",
            "Epoch 214 loss: 7.060590312100178\n",
            "Epoch 215 loss: 7.057268512071863\n",
            "Epoch 216 loss: 7.054479435793628\n",
            "Epoch 217 loss: 7.050989898723421\n",
            "Epoch 218 loss: 7.048228655593724\n",
            "Epoch 219 loss: 7.0438297318380965\n",
            "Epoch 220 loss: 7.042077951995437\n",
            "Epoch 221 loss: 7.039021357596375\n",
            "Epoch 222 loss: 7.034405712363802\n",
            "Epoch 223 loss: 7.031810690508729\n",
            "Epoch 224 loss: 7.028364548431303\n",
            "Epoch 225 loss: 7.0239348567355835\n",
            "Epoch 226 loss: 7.0219493311826175\n",
            "Epoch 227 loss: 7.019076635770567\n",
            "Epoch 228 loss: 7.013954434864563\n",
            "Epoch 229 loss: 7.0128392416385275\n",
            "Epoch 230 loss: 7.009755042466953\n",
            "Epoch 231 loss: 7.005956828274871\n",
            "Epoch 232 loss: 7.004253850644623\n",
            "Epoch 233 loss: 7.000223143039396\n",
            "Epoch 234 loss: 6.998632990672304\n",
            "Epoch 235 loss: 6.996003497593118\n",
            "Epoch 236 loss: 6.992717706633535\n",
            "Epoch 237 loss: 6.988303488767245\n",
            "Epoch 238 loss: 6.986759607488717\n",
            "Epoch 239 loss: 6.983157526619767\n",
            "Epoch 240 loss: 6.977968369317056\n",
            "Epoch 241 loss: 6.975366267428531\n",
            "Epoch 242 loss: 6.971790012298319\n",
            "Epoch 243 loss: 6.968986939598681\n",
            "Epoch 244 loss: 6.963641549580273\n",
            "Epoch 245 loss: 6.962765482299352\n",
            "Epoch 246 loss: 6.960931403425076\n",
            "Epoch 247 loss: 6.958376662428509\n",
            "Epoch 248 loss: 6.95631651176043\n",
            "Epoch 249 loss: 6.952814996188322\n",
            "Epoch 250 loss: 6.952993415277373\n",
            "Epoch 251 loss: 6.950251387951774\n",
            "Epoch 252 loss: 6.948213499303077\n",
            "Epoch 253 loss: 6.944876961205786\n",
            "Epoch 254 loss: 6.944035998434412\n",
            "Epoch 255 loss: 6.941148066305674\n",
            "Epoch 256 loss: 6.940902184050976\n",
            "Epoch 257 loss: 6.938644009627523\n",
            "Epoch 258 loss: 6.935431019960817\n",
            "Epoch 259 loss: 6.934596941145963\n",
            "Epoch 260 loss: 6.931788538444994\n",
            "Epoch 261 loss: 6.930883198115409\n",
            "Epoch 262 loss: 6.929244104855035\n",
            "Epoch 263 loss: 6.926874235038079\n",
            "Epoch 264 loss: 6.924234130595037\n",
            "Epoch 265 loss: 6.923591721460022\n",
            "Epoch 266 loss: 6.921580155309458\n",
            "Epoch 267 loss: 6.918696345598334\n",
            "Epoch 268 loss: 6.917445463239143\n",
            "Epoch 269 loss: 6.915648116790648\n",
            "Epoch 270 loss: 6.912627998192162\n",
            "Epoch 271 loss: 6.910205266800423\n",
            "Epoch 272 loss: 6.9068332026981105\n",
            "Epoch 273 loss: 6.906632990760852\n",
            "Epoch 274 loss: 6.904604753995555\n",
            "Epoch 275 loss: 6.902688028857714\n",
            "Epoch 276 loss: 6.900320684879209\n",
            "Epoch 277 loss: 6.897785671367644\n",
            "Epoch 278 loss: 6.897173104988471\n",
            "Epoch 279 loss: 6.894908953780524\n",
            "Epoch 280 loss: 6.893217875956233\n",
            "Epoch 281 loss: 6.890553468663963\n",
            "Epoch 282 loss: 6.890135138895384\n",
            "Epoch 283 loss: 6.888320105274543\n",
            "Epoch 284 loss: 6.885018875418662\n",
            "Epoch 285 loss: 6.883127682816087\n",
            "Epoch 286 loss: 6.882961700393366\n",
            "Epoch 287 loss: 6.880354907249259\n",
            "Epoch 288 loss: 6.879880444778377\n",
            "Epoch 289 loss: 6.878154460069035\n",
            "Epoch 290 loss: 6.8745001972000335\n",
            "Epoch 291 loss: 6.873471954348463\n",
            "Epoch 292 loss: 6.870677736182665\n",
            "Epoch 293 loss: 6.868990033182141\n",
            "Epoch 294 loss: 6.865748526878841\n",
            "Epoch 295 loss: 6.8663765726490595\n",
            "Epoch 296 loss: 6.863208552545747\n",
            "Epoch 297 loss: 6.861081708268055\n",
            "Epoch 298 loss: 6.860946528622\n",
            "Epoch 299 loss: 6.858183299548857\n",
            "Epoch 300 loss: 6.8574489890603765\n",
            "Epoch 301 loss: 6.856518684026976\n",
            "Epoch 302 loss: 6.8520197671112175\n",
            "Epoch 303 loss: 6.852766630651452\n",
            "Epoch 304 loss: 6.850784197187835\n",
            "Epoch 305 loss: 6.8475823450827376\n",
            "Epoch 306 loss: 6.848160406094678\n",
            "Epoch 307 loss: 6.8467879136034355\n",
            "Epoch 308 loss: 6.841864951724854\n",
            "Epoch 309 loss: 6.842491146670067\n",
            "Epoch 310 loss: 6.8394326303408395\n",
            "Epoch 311 loss: 6.83797200798018\n",
            "Epoch 312 loss: 6.837403430734065\n",
            "Epoch 313 loss: 6.835108543190907\n",
            "Epoch 314 loss: 6.832542129414897\n",
            "Epoch 315 loss: 6.833137078620415\n",
            "Epoch 316 loss: 6.272215272811936\n",
            "Epoch 317 loss: 6.25389290333365\n",
            "Epoch 318 loss: 6.254020747983054\n",
            "Epoch 319 loss: 6.251633910819934\n",
            "Epoch 320 loss: 6.233027535104485\n",
            "Epoch 321 loss: 6.2313604767607815\n",
            "Epoch 322 loss: 6.217937110106292\n",
            "Epoch 323 loss: 6.216386294992984\n",
            "Epoch 324 loss: 6.212443320832943\n",
            "Epoch 325 loss: 6.200442568202135\n",
            "Epoch 326 loss: 6.199188634683219\n",
            "Epoch 327 loss: 6.193490499696226\n",
            "Epoch 328 loss: 6.192217428511598\n",
            "Epoch 329 loss: 6.191197458125465\n",
            "Epoch 330 loss: 6.182732170443826\n",
            "Epoch 331 loss: 6.182826484815178\n",
            "Epoch 332 loss: 6.1748716592148885\n",
            "Epoch 333 loss: 6.173015579016916\n",
            "Epoch 334 loss: 6.1711414333311625\n",
            "Epoch 335 loss: 6.165306384551781\n",
            "Epoch 336 loss: 6.164131700136418\n",
            "Epoch 337 loss: 6.163638667397208\n",
            "Epoch 338 loss: 6.159723205265951\n",
            "Epoch 339 loss: 6.155959938904662\n",
            "Epoch 340 loss: 6.151600420499463\n",
            "Epoch 341 loss: 6.148132047043233\n",
            "Epoch 342 loss: 6.148271742777035\n",
            "Epoch 343 loss: 6.14604377362992\n",
            "Epoch 344 loss: 6.1433191687338065\n",
            "Epoch 345 loss: 6.142104712154946\n",
            "Epoch 346 loss: 6.1376784502292345\n",
            "Epoch 347 loss: 6.133810158141306\n",
            "Epoch 348 loss: 6.133653849127144\n",
            "Epoch 349 loss: 6.130400738330267\n",
            "Epoch 350 loss: 6.129653282059858\n",
            "Epoch 351 loss: 6.126505663335754\n",
            "Epoch 352 loss: 6.124853946952612\n",
            "Epoch 353 loss: 6.118957162526241\n",
            "Epoch 354 loss: 6.11788744872191\n",
            "Epoch 355 loss: 6.117915673946863\n",
            "Epoch 356 loss: 6.115735044171977\n",
            "Epoch 357 loss: 6.11280353113409\n",
            "Epoch 358 loss: 6.11260757865203\n",
            "Epoch 359 loss: 6.10899978003978\n",
            "Epoch 360 loss: 6.10834027860897\n",
            "Epoch 361 loss: 6.106586106011665\n",
            "Epoch 362 loss: 6.104934396883051\n",
            "Epoch 363 loss: 6.103922853852302\n",
            "Epoch 364 loss: 6.10093429540785\n",
            "Epoch 365 loss: 6.098385428982942\n",
            "Epoch 366 loss: 6.096664721894508\n",
            "Epoch 367 loss: 6.094559851265255\n",
            "Epoch 368 loss: 6.093963106152698\n",
            "Epoch 369 loss: 6.091523236167863\n",
            "Epoch 370 loss: 6.090775489473128\n",
            "Epoch 371 loss: 6.089640260003154\n",
            "Epoch 372 loss: 6.086337436961175\n",
            "Epoch 373 loss: 6.086064922763504\n",
            "Epoch 374 loss: 6.0846172025079515\n",
            "Epoch 375 loss: 6.082186913322621\n",
            "Epoch 376 loss: 6.078572341535949\n",
            "Epoch 377 loss: 6.078278499460486\n",
            "Epoch 378 loss: 6.075395806014128\n",
            "Epoch 379 loss: 6.074248002790221\n",
            "Epoch 380 loss: 6.072471385049001\n",
            "Epoch 381 loss: 6.072646748802335\n",
            "Epoch 382 loss: 6.06866729953306\n",
            "Epoch 383 loss: 6.066534274149695\n",
            "Epoch 384 loss: 6.066567014049566\n",
            "Epoch 385 loss: 6.065211330382699\n",
            "Epoch 386 loss: 6.062691069773316\n",
            "Epoch 387 loss: 6.061153416262797\n",
            "Epoch 388 loss: 6.059549975587668\n",
            "Epoch 389 loss: 6.057449527884192\n",
            "Epoch 390 loss: 6.055441366738375\n",
            "Epoch 391 loss: 6.053428939636409\n",
            "Epoch 392 loss: 6.053273015145924\n",
            "Epoch 393 loss: 6.048936296194652\n",
            "Epoch 394 loss: 6.049810377654841\n",
            "Epoch 395 loss: 6.0477256037902585\n",
            "Epoch 396 loss: 6.045269123735924\n",
            "Epoch 397 loss: 6.044474625059081\n",
            "Epoch 398 loss: 6.04214897649247\n",
            "Epoch 399 loss: 6.042307365281044\n",
            "Epoch 400 loss: 6.038038349051321\n",
            "Epoch 401 loss: 6.038127406738658\n",
            "Epoch 402 loss: 6.034416075787968\n",
            "Epoch 403 loss: 6.033941635023577\n",
            "Epoch 404 loss: 6.032744647095547\n",
            "Epoch 405 loss: 6.029907409909228\n",
            "Epoch 406 loss: 6.028884362848706\n",
            "Epoch 407 loss: 6.026729858527869\n",
            "Epoch 408 loss: 6.026336976916137\n",
            "Epoch 409 loss: 6.023704638788592\n",
            "Epoch 410 loss: 6.022714758360846\n",
            "Epoch 411 loss: 6.021386335196502\n",
            "Epoch 412 loss: 6.018641761477272\n",
            "Epoch 413 loss: 6.017207227839086\n",
            "Epoch 414 loss: 6.0148779690161085\n",
            "Epoch 415 loss: 6.0127397177825666\n",
            "Epoch 416 loss: 6.012428696616592\n",
            "Epoch 417 loss: 6.0095957621349605\n",
            "Epoch 418 loss: 6.009573066930948\n",
            "Epoch 419 loss: 6.007651117235987\n",
            "Epoch 420 loss: 6.006086359523577\n",
            "Epoch 421 loss: 6.0044496182697955\n",
            "Epoch 422 loss: 6.002382050536566\n",
            "Epoch 423 loss: 6.002568374049968\n",
            "Epoch 424 loss: 5.999994971313229\n",
            "Epoch 425 loss: 5.999366206324225\n",
            "Epoch 426 loss: 5.998348068198807\n",
            "Epoch 427 loss: 5.996374474217396\n",
            "Epoch 428 loss: 5.995503958651142\n",
            "Epoch 429 loss: 5.993619282841012\n",
            "Epoch 430 loss: 5.992371501282038\n",
            "Epoch 431 loss: 5.991634329109568\n",
            "Epoch 432 loss: 5.990458018406239\n",
            "Epoch 433 loss: 5.989513741026869\n",
            "Epoch 434 loss: 5.987931695088341\n",
            "Epoch 435 loss: 5.986004295366856\n",
            "Epoch 436 loss: 5.9863296397984165\n",
            "Epoch 437 loss: 5.984007092560113\n",
            "Epoch 438 loss: 5.981937709410798\n",
            "Epoch 439 loss: 5.981994429414871\n",
            "Epoch 440 loss: 5.979881082844462\n",
            "Epoch 441 loss: 5.978957815546646\n",
            "Epoch 442 loss: 5.977727367497651\n",
            "Epoch 443 loss: 5.975831333459762\n",
            "Epoch 444 loss: 5.975428882955981\n",
            "Epoch 445 loss: 5.972554987316918\n",
            "Epoch 446 loss: 5.972680840066148\n",
            "Epoch 447 loss: 5.970692665713141\n",
            "Epoch 448 loss: 5.969533508510459\n",
            "Epoch 449 loss: 5.968956374999235\n",
            "Epoch 450 loss: 5.966756038651289\n",
            "Epoch 451 loss: 5.96512266699774\n",
            "Epoch 452 loss: 5.965497239154968\n",
            "Epoch 453 loss: 5.962648003824639\n",
            "Epoch 454 loss: 5.962015442333904\n",
            "Epoch 455 loss: 5.961187103123057\n",
            "Epoch 456 loss: 5.959658199246714\n",
            "Epoch 457 loss: 5.958912118908802\n",
            "Epoch 458 loss: 5.95823061065234\n",
            "Epoch 459 loss: 5.955477266233473\n",
            "Epoch 460 loss: 5.954920941825132\n",
            "Epoch 461 loss: 5.953545460268632\n",
            "Epoch 462 loss: 5.951771650005349\n",
            "Epoch 463 loss: 5.952183706165503\n",
            "Epoch 464 loss: 5.950673322047515\n",
            "Epoch 465 loss: 5.948888630163818\n",
            "Epoch 466 loss: 5.948464771891562\n",
            "Epoch 467 loss: 5.947351926463327\n",
            "Epoch 468 loss: 5.945639717345069\n",
            "Epoch 469 loss: 5.9458435099408895\n",
            "Epoch 470 loss: 5.944289258558757\n",
            "Epoch 471 loss: 5.942969334064397\n",
            "Epoch 472 loss: 5.941435980801689\n",
            "Epoch 473 loss: 5.941530317401809\n",
            "Epoch 474 loss: 5.93978598554739\n",
            "Epoch 475 loss: 5.9393003267312245\n",
            "Epoch 476 loss: 5.938360674597717\n",
            "Epoch 477 loss: 5.937199167229672\n",
            "Epoch 478 loss: 5.935663188505487\n",
            "Epoch 479 loss: 5.934335512412892\n",
            "Epoch 480 loss: 5.934072814351406\n",
            "Epoch 481 loss: 5.933264109258301\n",
            "Epoch 482 loss: 5.931370567021868\n",
            "Epoch 483 loss: 5.931589760075304\n",
            "Epoch 484 loss: 5.930461059825316\n",
            "Epoch 485 loss: 5.9286298629367105\n",
            "Epoch 486 loss: 5.928514264340846\n",
            "Epoch 487 loss: 5.926986327459363\n",
            "Epoch 488 loss: 5.926228499266923\n",
            "Epoch 489 loss: 5.925592336379898\n",
            "Epoch 490 loss: 5.9234040181433185\n",
            "Epoch 491 loss: 5.923450092277983\n",
            "Epoch 492 loss: 5.923330458313113\n",
            "Epoch 493 loss: 5.921079705656272\n",
            "Epoch 494 loss: 5.920989981965296\n",
            "Epoch 495 loss: 5.918763416723873\n",
            "Epoch 496 loss: 5.918564548474424\n",
            "Epoch 497 loss: 5.917170726585545\n",
            "Epoch 498 loss: 5.916546889873722\n",
            "Epoch 499 loss: 5.915742266109989\n",
            "Epoch 500 loss: 5.91483779182008\n",
            "Epoch 501 loss: 5.914212726236135\n",
            "Epoch 502 loss: 5.913128114119422\n",
            "Epoch 503 loss: 5.911891038207873\n",
            "Epoch 504 loss: 5.9111311633011345\n",
            "Epoch 505 loss: 5.910263692991332\n",
            "Epoch 506 loss: 5.9095593572075895\n",
            "Epoch 507 loss: 5.909027356823826\n",
            "Epoch 508 loss: 5.907294994579017\n",
            "Epoch 509 loss: 5.907351755274686\n",
            "Epoch 510 loss: 5.906066724538665\n",
            "Epoch 511 loss: 5.905291552866226\n",
            "Epoch 512 loss: 5.904552803315939\n",
            "Epoch 513 loss: 5.904439880964695\n",
            "Epoch 514 loss: 5.901398983387179\n",
            "Epoch 515 loss: 5.9018698223846675\n",
            "Epoch 516 loss: 5.900298105641903\n",
            "Epoch 517 loss: 5.898813479975147\n",
            "Epoch 518 loss: 5.895655087390967\n",
            "Epoch 519 loss: 5.894195981022187\n",
            "Epoch 520 loss: 5.89113892823855\n",
            "Epoch 521 loss: 5.889283188082272\n",
            "Epoch 522 loss: 5.887389998147704\n",
            "Epoch 523 loss: 5.885471710450438\n",
            "Epoch 524 loss: 5.884245059385848\n",
            "Epoch 525 loss: 5.88097393338025\n",
            "Epoch 526 loss: 5.880221363767173\n",
            "Epoch 527 loss: 5.877606872956604\n",
            "Epoch 528 loss: 5.8771994960876635\n",
            "Epoch 529 loss: 5.875295511764429\n",
            "Epoch 530 loss: 5.874030118600318\n",
            "Epoch 531 loss: 5.87198611166718\n",
            "Epoch 532 loss: 5.871118007360103\n",
            "Epoch 533 loss: 5.870634314668496\n",
            "Epoch 534 loss: 5.86866970046459\n",
            "Epoch 535 loss: 5.866808772688938\n",
            "Epoch 536 loss: 5.865696826950856\n",
            "Epoch 537 loss: 5.8657776331770854\n",
            "Epoch 538 loss: 5.862939875471978\n",
            "Epoch 539 loss: 5.863044793604219\n",
            "Epoch 540 loss: 5.8615297316682815\n",
            "Epoch 541 loss: 5.860044903505138\n",
            "Epoch 542 loss: 5.8589932488285665\n",
            "Epoch 543 loss: 5.858933984794021\n",
            "Epoch 544 loss: 5.856343578416249\n",
            "Epoch 545 loss: 5.8566793454485815\n",
            "Epoch 546 loss: 5.854190871986483\n",
            "Epoch 547 loss: 5.854466520437458\n",
            "Epoch 548 loss: 5.8547170401581345\n",
            "Epoch 549 loss: 5.85103224249313\n",
            "Epoch 550 loss: 5.851786182020817\n",
            "Epoch 551 loss: 5.849200671275244\n",
            "Epoch 552 loss: 5.849182536763617\n",
            "Epoch 553 loss: 5.847294204696545\n",
            "Epoch 554 loss: 5.847773278759069\n",
            "Epoch 555 loss: 5.845444227771787\n",
            "Epoch 556 loss: 5.8453890666364465\n",
            "Epoch 557 loss: 5.845640660077798\n",
            "Epoch 558 loss: 5.842207246461983\n",
            "Epoch 559 loss: 5.842986853751069\n",
            "Epoch 560 loss: 5.840411761674573\n",
            "Epoch 561 loss: 5.84124604642658\n",
            "Epoch 562 loss: 5.8386961078950215\n",
            "Epoch 563 loss: 5.83792654613289\n",
            "Epoch 564 loss: 5.838058632204114\n",
            "Epoch 565 loss: 5.837511244805345\n",
            "Epoch 566 loss: 5.835496416247421\n",
            "Epoch 567 loss: 5.835796431661027\n",
            "Epoch 568 loss: 5.832419700062945\n",
            "Epoch 569 loss: 5.832365669714143\n",
            "Epoch 570 loss: 5.832314481702726\n",
            "Epoch 571 loss: 5.831107336305654\n",
            "Epoch 572 loss: 5.830394039065696\n",
            "Epoch 573 loss: 5.827291332956879\n",
            "Epoch 574 loss: 5.827938356795887\n",
            "Epoch 575 loss: 5.827864255612882\n",
            "Epoch 576 loss: 5.827166226500071\n",
            "Epoch 577 loss: 5.825199967852648\n",
            "Epoch 578 loss: 5.825336106238042\n",
            "Epoch 579 loss: 5.824690426516492\n",
            "Epoch 580 loss: 5.823008422795943\n",
            "Epoch 581 loss: 5.821327508598846\n",
            "Epoch 582 loss: 5.822158631785739\n",
            "Epoch 583 loss: 5.819459996940764\n",
            "Epoch 584 loss: 5.819606744706643\n",
            "Epoch 585 loss: 5.818959977692522\n",
            "Epoch 586 loss: 5.816653873416864\n",
            "Epoch 587 loss: 5.816826248852053\n",
            "Epoch 588 loss: 5.816419408831588\n",
            "Epoch 589 loss: 5.81382268559955\n",
            "Epoch 590 loss: 5.814609685784186\n",
            "Epoch 591 loss: 5.812192764231629\n",
            "Epoch 592 loss: 5.813053256715437\n",
            "Epoch 593 loss: 5.809955661939344\n",
            "Epoch 594 loss: 5.81049006240269\n",
            "Epoch 595 loss: 5.809716650731352\n",
            "Epoch 596 loss: 5.808204473904628\n",
            "Epoch 597 loss: 5.8078278171752045\n",
            "Epoch 598 loss: 5.806491883729289\n",
            "Epoch 599 loss: 5.80574763736065\n",
            "Epoch 600 loss: 5.804774784229976\n",
            "Epoch 601 loss: 5.805041873780726\n",
            "Epoch 602 loss: 5.803184228631316\n",
            "Epoch 603 loss: 5.804002978426172\n",
            "Epoch 604 loss: 5.80063228354887\n",
            "Epoch 605 loss: 5.8016476474531125\n",
            "Epoch 606 loss: 5.801377695942201\n",
            "Epoch 607 loss: 5.799454434205447\n",
            "Epoch 608 loss: 5.7996194792799525\n",
            "Epoch 609 loss: 5.798293933169493\n",
            "Epoch 610 loss: 5.798084646701783\n",
            "Epoch 611 loss: 5.79698335537252\n",
            "Epoch 612 loss: 5.797260199891237\n",
            "Epoch 613 loss: 5.795716819138001\n",
            "Epoch 614 loss: 5.7956808282541745\n",
            "Epoch 615 loss: 5.793940292765218\n",
            "Epoch 616 loss: 5.793695296726583\n",
            "Epoch 617 loss: 5.792669586083395\n",
            "Epoch 618 loss: 5.792646106617195\n",
            "Epoch 619 loss: 5.7908782233938485\n",
            "Epoch 620 loss: 5.792588823485525\n",
            "Epoch 621 loss: 5.790204379861378\n",
            "Epoch 622 loss: 5.790077145300129\n",
            "Epoch 623 loss: 5.789211870914887\n",
            "Epoch 624 loss: 5.78799764701451\n",
            "Epoch 625 loss: 5.78897801156397\n",
            "Epoch 626 loss: 5.787220194726781\n",
            "Epoch 627 loss: 5.786555903460527\n",
            "Epoch 628 loss: 5.78701971514558\n",
            "Epoch 629 loss: 5.785328246512436\n",
            "Epoch 630 loss: 5.785303819347958\n",
            "Epoch 631 loss: 5.784659772942089\n",
            "Epoch 632 loss: 5.782496978554539\n",
            "Epoch 633 loss: 5.782718796992825\n",
            "Epoch 634 loss: 5.781305419909722\n",
            "Epoch 635 loss: 5.781012609167757\n",
            "Epoch 636 loss: 5.779380428377641\n",
            "Epoch 637 loss: 5.779264088578902\n",
            "Epoch 638 loss: 5.777005083170793\n",
            "Epoch 639 loss: 5.77691488645316\n",
            "Epoch 640 loss: 5.777026269648861\n",
            "Epoch 641 loss: 5.775079706537646\n",
            "Epoch 642 loss: 5.775420486453261\n",
            "Epoch 643 loss: 5.77455468194316\n",
            "Epoch 644 loss: 5.772778593349859\n",
            "Epoch 645 loss: 5.774121879937215\n",
            "Epoch 646 loss: 5.772343789677034\n",
            "Epoch 647 loss: 5.771837042190831\n",
            "Epoch 648 loss: 5.769777802046514\n",
            "Epoch 649 loss: 5.770674378004649\n",
            "Epoch 650 loss: 5.768951502780034\n",
            "Epoch 651 loss: 5.768493107320419\n",
            "Epoch 652 loss: 5.768710041399453\n",
            "Epoch 653 loss: 5.766720374318809\n",
            "Epoch 654 loss: 5.767217147417534\n",
            "Epoch 655 loss: 5.765425462205109\n",
            "Epoch 656 loss: 5.765969070590976\n",
            "Epoch 657 loss: 5.764505756384929\n",
            "Epoch 658 loss: 5.76452488259636\n",
            "Epoch 659 loss: 5.763640387008695\n",
            "Epoch 660 loss: 5.763431415891605\n",
            "Epoch 661 loss: 5.763009200674581\n",
            "Epoch 662 loss: 5.761324921451198\n",
            "Epoch 663 loss: 5.762087263100116\n",
            "Epoch 664 loss: 5.759508480697885\n",
            "Epoch 665 loss: 5.760112631990826\n",
            "Epoch 666 loss: 5.7586273363350875\n",
            "Epoch 667 loss: 5.757843698638801\n",
            "Epoch 668 loss: 5.758843474737525\n",
            "Epoch 669 loss: 5.756619323218\n",
            "Epoch 670 loss: 5.757111908038324\n",
            "Epoch 671 loss: 5.754808849578002\n",
            "Epoch 672 loss: 5.7559437891512015\n",
            "Epoch 673 loss: 5.753934462241336\n",
            "Epoch 674 loss: 5.754271921941856\n",
            "Epoch 675 loss: 5.752380607128799\n",
            "Epoch 676 loss: 5.752280280645085\n",
            "Epoch 677 loss: 5.750367377327727\n",
            "Epoch 678 loss: 5.750428701491787\n",
            "Epoch 679 loss: 5.750929284597309\n",
            "Epoch 680 loss: 5.749718830812099\n",
            "Epoch 681 loss: 5.749767820397446\n",
            "Epoch 682 loss: 5.747459494271794\n",
            "Epoch 683 loss: 5.747202914027254\n",
            "Epoch 684 loss: 5.747510485249013\n",
            "Epoch 685 loss: 5.747226985616319\n",
            "Epoch 686 loss: 5.745781814423131\n",
            "Epoch 687 loss: 5.745190376205171\n",
            "Epoch 688 loss: 5.745539471083017\n",
            "Epoch 689 loss: 5.745098612829491\n",
            "Epoch 690 loss: 5.744466494276818\n",
            "Epoch 691 loss: 5.742762150516196\n",
            "Epoch 692 loss: 5.7437847263796575\n",
            "Epoch 693 loss: 5.74232835146606\n",
            "Epoch 694 loss: 5.742499814337408\n",
            "Epoch 695 loss: 5.740649143977333\n",
            "Epoch 696 loss: 5.740995094632804\n",
            "Epoch 697 loss: 5.741377611130974\n",
            "Epoch 698 loss: 5.739689112482792\n",
            "Epoch 699 loss: 5.740141665437983\n",
            "Epoch 700 loss: 5.737518291001966\n",
            "Epoch 701 loss: 5.738995016325465\n",
            "Epoch 702 loss: 5.737577593136198\n",
            "Epoch 703 loss: 5.737775039896416\n",
            "Epoch 704 loss: 5.736644489392864\n",
            "Epoch 705 loss: 5.736006196927324\n",
            "Epoch 706 loss: 5.736750360366859\n",
            "Epoch 707 loss: 5.735440811954491\n",
            "Epoch 708 loss: 5.734692250101716\n",
            "Epoch 709 loss: 5.734698453857188\n",
            "Epoch 710 loss: 5.732879123353011\n",
            "Epoch 711 loss: 5.733208954567253\n",
            "Epoch 712 loss: 5.731834073547819\n",
            "Epoch 713 loss: 5.732029288350194\n",
            "Epoch 714 loss: 5.731787792875269\n",
            "Epoch 715 loss: 5.731652251365166\n",
            "Epoch 716 loss: 5.730865868978124\n",
            "Epoch 717 loss: 5.728728387731302\n",
            "Epoch 718 loss: 5.730460478624133\n",
            "Epoch 719 loss: 5.728238509937065\n",
            "Epoch 720 loss: 5.728294089906914\n",
            "Epoch 721 loss: 5.727817744856094\n",
            "Epoch 722 loss: 5.7249081817330785\n",
            "Epoch 723 loss: 5.7255071598600855\n",
            "Epoch 724 loss: 5.723865385023677\n",
            "Epoch 725 loss: 5.724319734044688\n",
            "Epoch 726 loss: 5.721752073896018\n",
            "Epoch 727 loss: 5.7228618517374095\n",
            "Epoch 728 loss: 5.72224499782909\n",
            "Epoch 729 loss: 5.720642076817001\n",
            "Epoch 730 loss: 5.721473789069092\n",
            "Epoch 731 loss: 5.7200362924069506\n",
            "Epoch 732 loss: 5.720234228789096\n",
            "Epoch 733 loss: 5.719879240729963\n",
            "Epoch 734 loss: 5.718182311930027\n",
            "Epoch 735 loss: 5.719117613447533\n",
            "Epoch 736 loss: 5.71817024145736\n",
            "Epoch 737 loss: 5.716839006345383\n",
            "Epoch 738 loss: 5.718984001063977\n",
            "Epoch 739 loss: 5.716571691331683\n",
            "Epoch 740 loss: 5.716413881123934\n",
            "Epoch 741 loss: 5.716740210387312\n",
            "Epoch 742 loss: 5.714437908963434\n",
            "Epoch 743 loss: 5.7154435380873245\n",
            "Epoch 744 loss: 5.714795612820025\n",
            "Epoch 745 loss: 5.713990073891492\n",
            "Epoch 746 loss: 5.712909766112421\n",
            "Epoch 747 loss: 5.713362539580773\n",
            "Epoch 748 loss: 5.711320390553254\n",
            "Epoch 749 loss: 5.712683455950113\n",
            "Epoch 750 loss: 5.711122585314376\n",
            "Epoch 751 loss: 5.711249916449365\n",
            "Epoch 752 loss: 5.7096521146832435\n",
            "Epoch 753 loss: 5.709037883882298\n",
            "Epoch 754 loss: 5.709019619474692\n",
            "Epoch 755 loss: 5.706674871307648\n",
            "Epoch 756 loss: 5.708523855099712\n",
            "Epoch 757 loss: 5.705252544693659\n",
            "Epoch 758 loss: 5.706000762266585\n",
            "Epoch 759 loss: 5.7062951861333016\n",
            "Epoch 760 loss: 5.705276946597704\n",
            "Epoch 761 loss: 5.704840009490268\n",
            "Epoch 762 loss: 5.704144440727407\n",
            "Epoch 763 loss: 5.703704310396097\n",
            "Epoch 764 loss: 5.701935989051217\n",
            "Epoch 765 loss: 5.703273348322151\n",
            "Epoch 766 loss: 5.700764363590885\n",
            "Epoch 767 loss: 5.702099804958118\n",
            "Epoch 768 loss: 5.7012886056572105\n",
            "Epoch 769 loss: 5.700509547844126\n",
            "Epoch 770 loss: 5.699246235227891\n",
            "Epoch 771 loss: 5.699374486763283\n",
            "Epoch 772 loss: 5.699300222406698\n",
            "Epoch 773 loss: 5.697539189226399\n",
            "Epoch 774 loss: 5.697882382866924\n",
            "Epoch 775 loss: 5.697917585213968\n",
            "Epoch 776 loss: 5.696581591124646\n",
            "Epoch 777 loss: 5.696310307247746\n",
            "Epoch 778 loss: 5.696218178185153\n",
            "Epoch 779 loss: 5.694166857477536\n",
            "Epoch 780 loss: 5.6949788824232535\n",
            "Epoch 781 loss: 5.693292425522238\n",
            "Epoch 782 loss: 5.694391783107508\n",
            "Epoch 783 loss: 5.692785479088897\n",
            "Epoch 784 loss: 5.693004792769707\n",
            "Epoch 785 loss: 5.6923517889476685\n",
            "Epoch 786 loss: 5.6915378893898385\n",
            "Epoch 787 loss: 5.6924512061667185\n",
            "Epoch 788 loss: 5.691552166375213\n",
            "Epoch 789 loss: 5.690815777390572\n",
            "Epoch 790 loss: 5.690738669244391\n",
            "Epoch 791 loss: 5.690940310907117\n",
            "Epoch 792 loss: 5.689142548007169\n",
            "Epoch 793 loss: 5.689811208813648\n",
            "Epoch 794 loss: 5.688954422068447\n",
            "Epoch 795 loss: 5.689034389079977\n",
            "Epoch 796 loss: 5.68932033181123\n",
            "Epoch 797 loss: 5.6874046973304\n",
            "Epoch 798 loss: 5.686915292710193\n",
            "Epoch 799 loss: 5.686570773311758\n",
            "Epoch 800 loss: 5.686795001135784\n",
            "Epoch 801 loss: 5.686391478260166\n",
            "Epoch 802 loss: 5.68562572113451\n",
            "Epoch 803 loss: 5.685355257292811\n",
            "Epoch 804 loss: 5.684658170640237\n",
            "Epoch 805 loss: 5.68540807047739\n",
            "Epoch 806 loss: 5.684391854659555\n",
            "Epoch 807 loss: 5.684572613942765\n",
            "Epoch 808 loss: 5.684669734179533\n",
            "Epoch 809 loss: 5.683612215739544\n",
            "Epoch 810 loss: 5.683208539048874\n",
            "Epoch 811 loss: 5.6820861067266115\n",
            "Epoch 812 loss: 5.682593847589771\n",
            "Epoch 813 loss: 5.682302731903057\n",
            "Epoch 814 loss: 5.680952127362891\n",
            "Epoch 815 loss: 5.680743895751238\n",
            "Epoch 816 loss: 5.68080532919485\n",
            "Epoch 817 loss: 5.680426316982269\n",
            "Epoch 818 loss: 5.680065360727263\n",
            "Epoch 819 loss: 5.678491279622209\n",
            "Epoch 820 loss: 5.678597411612399\n",
            "Epoch 821 loss: 5.678952586808788\n",
            "Epoch 822 loss: 5.67760209899084\n",
            "Epoch 823 loss: 5.6775544616733225\n",
            "Epoch 824 loss: 5.677019623965338\n",
            "Epoch 825 loss: 5.675904418631462\n",
            "Epoch 826 loss: 5.674794636424264\n",
            "Epoch 827 loss: 5.674236607417319\n",
            "Epoch 828 loss: 5.674009427512718\n",
            "Epoch 829 loss: 5.673558832608416\n",
            "Epoch 830 loss: 5.672352077012152\n",
            "Epoch 831 loss: 5.67219686876417\n",
            "Epoch 832 loss: 5.671391161322589\n",
            "Epoch 833 loss: 5.670484733519497\n",
            "Epoch 834 loss: 5.6700511498762385\n",
            "Epoch 835 loss: 5.670584233965743\n",
            "Epoch 836 loss: 5.669830744526818\n",
            "Epoch 837 loss: 5.667874450843434\n",
            "Epoch 838 loss: 5.668698783554055\n",
            "Epoch 839 loss: 5.667802624833443\n",
            "Epoch 840 loss: 5.667763803210975\n",
            "Epoch 841 loss: 5.666811293769315\n",
            "Epoch 842 loss: 5.666876658156266\n",
            "Epoch 843 loss: 5.666575102061874\n",
            "Epoch 844 loss: 5.665521563321203\n",
            "Epoch 845 loss: 5.665506993391701\n",
            "Epoch 846 loss: 5.664639348355632\n",
            "Epoch 847 loss: 5.664724834727838\n",
            "Epoch 848 loss: 5.6645542142722105\n",
            "Epoch 849 loss: 5.663616998243648\n",
            "Epoch 850 loss: 5.663842925331962\n",
            "Epoch 851 loss: 5.664708921946252\n",
            "Epoch 852 loss: 5.663384969325254\n",
            "Epoch 853 loss: 5.662824626704758\n",
            "Epoch 854 loss: 5.662709673322181\n",
            "Epoch 855 loss: 5.661766067644632\n",
            "Epoch 856 loss: 5.661785230302424\n",
            "Epoch 857 loss: 5.66193049938241\n",
            "Epoch 858 loss: 5.661215423796109\n",
            "Epoch 859 loss: 5.660228893865586\n",
            "Epoch 860 loss: 5.660633654786169\n",
            "Epoch 861 loss: 5.659380120253527\n",
            "Epoch 862 loss: 5.659753043861388\n",
            "Epoch 863 loss: 5.658526789575267\n",
            "Epoch 864 loss: 5.65910272845113\n",
            "Epoch 865 loss: 5.6588844091245125\n",
            "Epoch 866 loss: 5.657792290427473\n",
            "Epoch 867 loss: 5.657548560943853\n",
            "Epoch 868 loss: 5.657335882600795\n",
            "Epoch 869 loss: 5.65619115855095\n",
            "Epoch 870 loss: 5.656192304204648\n",
            "Epoch 871 loss: 5.655083998221139\n",
            "Epoch 872 loss: 5.655390022244832\n",
            "Epoch 873 loss: 5.654823179526919\n",
            "Epoch 874 loss: 5.653830728080247\n",
            "Epoch 875 loss: 5.654135259700833\n",
            "Epoch 876 loss: 5.6543955880373\n",
            "Epoch 877 loss: 5.652237866943352\n",
            "Epoch 878 loss: 5.652576565533085\n",
            "Epoch 879 loss: 5.652329645555543\n",
            "Epoch 880 loss: 5.651305361793685\n",
            "Epoch 881 loss: 5.651049526439029\n",
            "Epoch 882 loss: 5.650983564162439\n",
            "Epoch 883 loss: 5.6496925389484405\n",
            "Epoch 884 loss: 5.649558193860342\n",
            "Epoch 885 loss: 5.649890676875966\n",
            "Epoch 886 loss: 5.648530557622037\n",
            "Epoch 887 loss: 5.648728717962355\n",
            "Epoch 888 loss: 5.648423967246124\n",
            "Epoch 889 loss: 5.648006422482011\n",
            "Epoch 890 loss: 5.64685382055626\n",
            "Epoch 891 loss: 5.646726480750893\n",
            "Epoch 892 loss: 5.64696290370242\n",
            "Epoch 893 loss: 5.647269968181531\n",
            "Epoch 894 loss: 5.6459380403484545\n",
            "Epoch 895 loss: 5.6459141828861386\n",
            "Epoch 896 loss: 5.645835474105482\n",
            "Epoch 897 loss: 5.645245253372083\n",
            "Epoch 898 loss: 5.64474687645722\n",
            "Epoch 899 loss: 5.644948059909474\n",
            "Epoch 900 loss: 5.644259372786552\n",
            "Epoch 901 loss: 5.643173155417554\n",
            "Epoch 902 loss: 5.64326734039044\n",
            "Epoch 903 loss: 5.642518859123554\n",
            "Epoch 904 loss: 5.642591418418292\n",
            "Epoch 905 loss: 5.641760806071543\n",
            "Epoch 906 loss: 5.64200260988614\n",
            "Epoch 907 loss: 5.641701849353054\n",
            "Epoch 908 loss: 5.640490218657216\n",
            "Epoch 909 loss: 5.64041874899048\n",
            "Epoch 910 loss: 5.639339218744021\n",
            "Epoch 911 loss: 5.6391630509036474\n",
            "Epoch 912 loss: 5.63767583406815\n",
            "Epoch 913 loss: 5.637503951536328\n",
            "Epoch 914 loss: 5.6377905440853615\n",
            "Epoch 915 loss: 5.6375386092377795\n",
            "Epoch 916 loss: 5.63687439776533\n",
            "Epoch 917 loss: 5.635566654456791\n",
            "Epoch 918 loss: 5.635216208410806\n",
            "Epoch 919 loss: 5.634943576541606\n",
            "Epoch 920 loss: 5.634670888398456\n",
            "Epoch 921 loss: 5.6336146950698724\n",
            "Epoch 922 loss: 5.633696010542464\n",
            "Epoch 923 loss: 5.6326188173423475\n",
            "Epoch 924 loss: 5.632313558148819\n",
            "Epoch 925 loss: 5.631197538980059\n",
            "Epoch 926 loss: 5.632012576909603\n",
            "Epoch 927 loss: 5.631114535163414\n",
            "Epoch 928 loss: 5.630481837698523\n",
            "Epoch 929 loss: 5.629602137768722\n",
            "Epoch 930 loss: 5.628996316435733\n",
            "Epoch 931 loss: 5.629084577234064\n",
            "Epoch 932 loss: 5.6286659145584\n",
            "Epoch 933 loss: 5.6276930908518805\n",
            "Epoch 934 loss: 5.627528486575144\n",
            "Epoch 935 loss: 5.626898879500157\n",
            "Epoch 936 loss: 5.626778286792299\n",
            "Epoch 937 loss: 5.626069781948685\n",
            "Epoch 938 loss: 5.624849696310319\n",
            "Epoch 939 loss: 5.624876215764073\n",
            "Epoch 940 loss: 5.623960921491349\n",
            "Epoch 941 loss: 5.624046466247697\n",
            "Epoch 942 loss: 5.623333553646621\n",
            "Epoch 943 loss: 5.622559721273232\n",
            "Epoch 944 loss: 5.622148422227943\n",
            "Epoch 945 loss: 5.621298376135353\n",
            "Epoch 946 loss: 5.6205592418789445\n",
            "Epoch 947 loss: 5.619333156376913\n",
            "Epoch 948 loss: 5.619749956206955\n",
            "Epoch 949 loss: 5.618543456684692\n",
            "Epoch 950 loss: 5.618921668649874\n",
            "Epoch 951 loss: 5.618068331291827\n",
            "Epoch 952 loss: 5.618244252868016\n",
            "Epoch 953 loss: 5.616772620647936\n",
            "Epoch 954 loss: 5.616567720638624\n",
            "Epoch 955 loss: 5.61585010307695\n",
            "Epoch 956 loss: 5.614736091088824\n",
            "Epoch 957 loss: 5.614230741479474\n",
            "Epoch 958 loss: 5.613383864305328\n",
            "Epoch 959 loss: 5.614202367434954\n",
            "Epoch 960 loss: 5.613209228241299\n",
            "Epoch 961 loss: 5.612484090595412\n",
            "Epoch 962 loss: 5.611794680678749\n",
            "Epoch 963 loss: 5.611085123859021\n",
            "Epoch 964 loss: 5.610393347295709\n",
            "Epoch 965 loss: 5.610419482389464\n",
            "Epoch 966 loss: 5.61048380122105\n",
            "Epoch 967 loss: 5.6093318251950555\n",
            "Epoch 968 loss: 5.6086675220161215\n",
            "Epoch 969 loss: 5.607905300795983\n",
            "Epoch 970 loss: 5.608615146591737\n",
            "Epoch 971 loss: 5.608078550552348\n",
            "Epoch 972 loss: 5.607400024119408\n",
            "Epoch 973 loss: 5.606324066755083\n",
            "Epoch 974 loss: 5.60598443011661\n",
            "Epoch 975 loss: 5.6058957451059594\n",
            "Epoch 976 loss: 5.605207883350983\n",
            "Epoch 977 loss: 5.604588321375704\n",
            "Epoch 978 loss: 5.604555000462447\n",
            "Epoch 979 loss: 5.6042270693410705\n",
            "Epoch 980 loss: 5.603232487512087\n",
            "Epoch 981 loss: 5.602999104567022\n",
            "Epoch 982 loss: 5.6022790456500235\n",
            "Epoch 983 loss: 5.601973076545649\n",
            "Epoch 984 loss: 5.600556353724164\n",
            "Epoch 985 loss: 5.600470560620723\n",
            "Epoch 986 loss: 5.599852259958397\n",
            "Epoch 987 loss: 5.599895698349445\n",
            "Epoch 988 loss: 5.599362284743056\n",
            "Epoch 989 loss: 5.599277435621306\n",
            "Epoch 990 loss: 5.599256979556025\n",
            "Epoch 991 loss: 5.597768347845185\n",
            "Epoch 992 loss: 5.5977684576046185\n",
            "Epoch 993 loss: 5.597078433608559\n",
            "Epoch 994 loss: 5.597831462400361\n",
            "Epoch 995 loss: 5.596787848221432\n",
            "Epoch 996 loss: 5.59616112277752\n",
            "Epoch 997 loss: 5.595863242062112\n",
            "Epoch 998 loss: 5.595060613838192\n",
            "Epoch 999 loss: 5.594914140292158\n",
            "Epoch 1000 loss: 5.59523098566785\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score"
      ],
      "metadata": {
        "id": "8Po-_udQVT2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(model, X_test, y_test_encoded, y_test):\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    for i in range(X_test.shape[0]):\n",
        "        # Forward pass\n",
        "        output = X_test[i]\n",
        "        for layer in model:\n",
        "            output = layer.forward(output)\n",
        "\n",
        "        # Get predicted class (index of max probability)\n",
        "        predicted_class = np.argmax(output)\n",
        "        true_class = np.argmax(y_test_encoded[i])\n",
        "\n",
        "        # Store values for metrics calculation\n",
        "        y_true.append(true_class)\n",
        "        y_pred.append(predicted_class)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = (np.array(y_true) == np.array(y_pred)).mean() * 100\n",
        "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "    # Calculate precision and recall for each class\n",
        "    num_classes = y_test_encoded.shape[1]\n",
        "    for cls in range(num_classes):\n",
        "        precision = precision_score(y_true, y_pred, labels=[cls], average=\"macro\", zero_division=0)\n",
        "        recall = recall_score(y_true, y_pred, labels=[cls], average=\"macro\", zero_division=0)\n",
        "        print(f\"Class {cls}: Precision = {precision:.2f}, Recall = {recall:.2f}\")\n"
      ],
      "metadata": {
        "id": "1bke-jsdUye4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_model(model, X_test, y_test_encoded, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Z582GViU0RU",
        "outputId": "8a7f0b8d-7a92-4f9f-bc1b-0b90c2601403"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 79.02%\n",
            "Class 0: Precision = 0.78, Recall = 0.86\n",
            "Class 1: Precision = 0.80, Recall = 0.70\n"
          ]
        }
      ]
    }
  ]
}