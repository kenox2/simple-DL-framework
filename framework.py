# -*- coding: utf-8 -*-
"""Untitled6.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QWN1gK7DrCrgc-qHoP0UVmDPZlPoMuWP
"""

# Loss functions and gradients
class Loss:
    @staticmethod
    def cross_entropy(y_true, y_pred):
        # Clip values to avoid log(0) issues
        epsilon = 1e-12
        y_pred = np.clip(y_pred, epsilon, 1. - epsilon)
        loss = -np.sum(y_true * np.log(y_pred))
        return loss

    @staticmethod
    def cross_entropy_gradient(y_true, y_pred):
        return y_pred - y_true

# Activation functions and derivatives
class Activation:
    @staticmethod
    def relu(z):
        return np.maximum(0, z)

    @staticmethod
    def relu_derivative(z):
        return np.where(z > 0, 1, 0)

    @staticmethod
    def softmax(z):
        # Subtract max for numerical stability
        z -= np.max(z, axis=1, keepdims=True)
        exp_vals = np.exp(z)
        return exp_vals / np.sum(exp_vals, axis=1, keepdims=True)

    # For softmax used with cross-entropy, the derivative is taken care of in the loss gradient

# Fully connected layer definition
class Layer:
    def __init__(self, num_in, num_out, activation, activation_deriv, learning_rate=0.01):
        rng = np.random.default_rng(42)
        # He initialization for layers using ReLU
        self.weights = rng.standard_normal((num_in, num_out)) * np.sqrt(2 / num_in)
        self.biases = np.zeros((1, num_out))
        self.activation = activation
        self.activation_deriv = activation_deriv
        self.learning_rate = learning_rate
        self.inputs = None
        self.z = None

    def forward(self, inputs):
        self.inputs = inputs
        self.z = np.dot(inputs, self.weights) + self.biases
        return self.activation(self.z)

    def backward(self, dl_dy):
        # For layers with ReLU or similar activations
        if self.activation_deriv is not None:
            dl_dz = dl_dy * self.activation_deriv(self.z)
        else:
            dl_dz = dl_dy
        dl_dw = self.inputs.T @ dl_dz
        dl_db = np.sum(dl_dz, axis=0, keepdims=True)
        dl_dx = np.dot(dl_dz, self.weights.T)

        batch_size = self.inputs.shape[0]
        self.weights -= self.learning_rate * (dl_dw / batch_size)
        self.biases  -= self.learning_rate * (dl_db / batch_size)
        return dl_dx



